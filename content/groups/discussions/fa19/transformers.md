---
title: Attention is All You Need
linktitle: Attention is All You Need

date: '2019-10-15T17:30:00'
lastmod: <?UNK?>

draft: false
toc: true

weight: 5

menu:
  discussions_fa19:
    parent: Fall 2019

authors: [ionlights, ahl98]

urls:
  youtube: ''
  slides: ''
  github: ''
  kaggle: ''
  colab: ''

papers:
  transformer: https://github.com/ucfai/discussions/raw/master/fa19/transformer.pdf
location: HPA1 117
cover: ''

categories: [fa19]
tags: [conference paper, deep learning, nlp, transformers, attention, seminal work]
abstract: >-
  This paper, published from work performed at Google Brain and Google Research,
  proposes a new network architecture for tackling machine translation problems
  (among other ML transduction problems). This new approach simplifies the classic
  approach to translation while also achieving better performance. Accompanying the  paper
  is a Jupyter notebook created at Harvard to add annotations to the original  article
  while also supplying code mentioned in the work. This paper is most similar
  to the kinds of articles you can expect to be reading when doing original research.

---

<!-- TODO Add Meeting Notes/Contents here -->
<!-- NOTE Refer the Documentation if you're unsure how to format/add to this. -->

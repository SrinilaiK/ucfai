---
title: Deep Visual-Semantic Alignments for Generating Image Descriptions
linktitle: Deep Visual-Semantic Alignments for Generating Image Descriptions

date: '2019-03-25T17:30:00'
lastmod: <?UNK?>

draft: false
toc: true

weight: 7

menu:
  discussions_sp19:
    parent: Spring 2019

authors: [waldmannly, ionlights]

urls:
  youtube: ''
  slides: ''
  github: ''
  kaggle: ''
  colab: ''

papers:
  image-caption-generation: https://github.com/ucfai/discussions/raw/master/sp19/image-caption-generation.pdf
location: HEC 119
cover: ''

categories: [sp19]
tags: [deep learning, convolutional networks, caption generation, image recognition]
abstract: >-
  Abstract: We present a model that generates natural language descriptions of
  images and their regions. Our approach leverages datasets of images and their
  sentence descriptions tolearn about the inter-modal correspondences between  language
  and visual data. Our alignment model is based on a novel combination
  of Convolutional Neural Networks over image regions, bidirectional Recurrent
  Neural Networks over sentences, and a structured objective that aligns the  two
  modalities through a multimodal embedding. We then describe a Multimodal
  Recurrent Neural Network architecture that uses the inferred alignments to
  learn to generate novel descriptions of image regions.  We demonstrate that  our
  alignment model produces state of the art results in retrieval experiments
  on Flickr8K, Flickr30K and MSCOCOdatasets. We then show that the generated
  descriptions sig outperform retrieval baselines on both full images and on a
  new dataset of region-level annotations.

---

<!-- TODO Add Meeting Notes/Contents here -->
<!-- NOTE Refer the Documentation if you're unsure how to format/add to this. -->
